image: carvicsforth/arch_carv

stages:
  - style
  - build
  - test
  - controller
  - build_all
  - docs
  - publish
  - publish_controller
  - trigger_deps
  - report
  - release

.build_base: &build_base
  stage: build
  needs: []

.build_all: &build_all
  stage: build_all
  needs: []
  when: manual

.mutext_utest: &mutext_utest
  stage: test
  needs: [build_mutex]
  dependencies:
    - build_mutex
  retry: 2

.only_master: &only_master
  only:
    refs:
      - master
  except:
    variables:
      - $CI_PIPELINE_SOURCE == "schedule"

style_check:
  stage: style
  rules:
    - if: $CI_PIPELINE_SOURCE == "push"
  needs: []
  script:
    - mkdir -p build
    - cd build; cmake ../;make pre-commit-all

report_stale_branches:
  stage: report
  needs: []
  script:
    - python ./ci_scripts/report_stale_branches.py
  when: always

build_spin:
  <<: *build_base
  script:
    - mkdir -p build
    - cd build; cmake -DDEBUG=ON -DARAX_CONFIG_FILE="$PWD/../araxconf" -Dasync_architecture=spin ../; make -j 4; cd ../
    - ./ci_scripts/pack.sh 0 . spin
  artifacts:
    paths:
      - spin.tgz
    expire_in: 2 hour
  when: manual

build_mutex:
  <<: *build_base
  script:
    - mkdir -p build
    - cd build; cmake -DCMAKE_BUILD_TYPE=DEBUG -DDEBUG=ON -DARAX_CONFIG_FILE="$PWD/../araxconf" -Dasync_architecture=mutex ../; make -j 4; cd ../
    - ./ci_scripts/pack.sh 0 . mutex
  artifacts:
    paths:
      - mutex.tgz
    expire_in: 24 hour

build_ivshmem:
  <<: *build_base
  script:
    - mkdir -p build
    - cd build; cmake -DDEBUG=ON -DARAX_CONFIG_FILE="$PWD/../araxconf" -Dasync_architecture=ivshmem ../; make -j 4; cd ../
    - ./ci_scripts/pack.sh 0 . ivshmem
  artifacts:
    paths:
      - ivshmem.tgz
    expire_in: 2 hour
  when: manual

build_all_spin:
  <<: *build_all
  script:
    - export gen_art=spin;./tools/build_check.sh spin

build_all_mutex:
  <<: *build_all
  script:
    - export gen_art=mutex;./tools/build_check.sh mutex

build_all_ivshmem:
  <<: *build_all
  script:
    - export gen_art=ivshmem;./tools/build_check.sh ivshmem

utest_spin:
  stage: test
  needs: [build_spin]
  script:
    - ./ci_scripts/pack.sh 1 . spin
    - cd build && ctest -V -W
  dependencies:
    - build_spin
  when: manual

utest_mutex:
  <<: *mutext_utest
  script:
    - ulimit -c 0
    - ./ci_scripts/pack.sh 1 . mutex
    - cd build && ctest -V -W

utest_ivshmem:
  stage: test
  needs: [build_ivshmem]
  script:
    - ./ci_scripts/pack.sh 1 . ivshmem
    - cd build && ctest -V -W
  dependencies:
    - build_ivshmem
  when: manual

coverage_mutex:
  <<: *mutext_utest
  script:
    - ./ci_scripts/pack.sh 1 . mutex
    - cd build
    - echo -e "\e[0Ksection_start:`date +%s`:cmake[collapsed=true]\r\e[0KCMake"
    - cmake .. -DCOVERAGE=ON -DBUILD_TESTS=ON
    - echo -e "\e[0Ksection_end:`date +%s`:cmake\r\e[0K"
    - echo -e "\e[0Ksection_start:`date +%s`:make[collapsed=true]\r\e[0KMake"
    - make -j 4
    - echo -e "\e[0Ksection_end:`date +%s`:make\r\e[0K"
    - make cov
    - pip install htmlark
    - htmlark -o cov.html ./coverage/coverage.html
  artifacts:
    paths:
      - build/cov.html
    expire_in: 7 Day

pahole:
  <<: *mutext_utest
  script:
    - ./ci_scripts/pack.sh 1 . mutex
    - python ./tools/pahole.py

build_doc:
  stage: build
  needs: []
  script:
    - mkdir -p build
    - cd build
    - cmake ../
    - make doc
    - cd docs
    - mv html AraxDocs
    - zip -gr AraxDocs.zip AraxDocs/*
  artifacts:
    paths:
      - build/docs/AraxDocs.zip
    expire_in: 1 Day
  only:
    refs:
      - master

publish_doc:
  <<: *only_master
  stage: publish
  needs: [build_doc]
  script:
    - bash ci_scripts/pub_doc.sh
  dependencies:
    - build_doc

publish_docker:
  stage: publish
  variables:
      BUILDAH_LAYERS: "true"
      DEVEL_TAG: "${DOCK_HOST}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}-devel:${CI_BUILD_REF_NAME}"
      RUNTIME_TAG: "${DOCK_HOST}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}-runtime:${CI_BUILD_REF_NAME}"
      INSTALL_TAG: "${DOCK_HOST}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}-install:${CI_BUILD_REF_NAME}"
  needs: [utest_mutex,controller]
  script:
    - echo "${DOCK_PASS}" | buildah login -u "${DOCK_USER}" --password-stdin ${DOCK_HOST}
    - buildah build --target ${CI_PROJECT_NAME}-devel -t ${DEVEL_TAG} -f ./docker/Dockerfile .
    - buildah build --target ${CI_PROJECT_NAME}-runtime -t ${RUNTIME_TAG} -f ./docker/Dockerfile .
    - buildah build --target ${CI_PROJECT_NAME}-install -t ${INSTALL_TAG} -f ./docker/Dockerfile .
    - buildah push ${DEVEL_TAG}
    - buildah push ${RUNTIME_TAG}
    - buildah push ${INSTALL_TAG}
    - buildah rm -a
    - buildah rmi -a

publish_controller_docker:
  stage: publish_controller
  needs: [publish_docker]
  variables:
      BUILDAH_LAYERS: "true"
      DEVEL_TAG: "${DOCK_HOST}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}controller-devel:${CI_BUILD_REF_NAME}"
      RUNTIME_TAG: "${DOCK_HOST}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}controller-runtime:${CI_BUILD_REF_NAME}"
      INSTALL_TAG: "${DOCK_HOST}/${CI_PROJECT_NAMESPACE}/${CI_PROJECT_NAME}controller-install:${CI_BUILD_REF_NAME}"
  script:
    - cd controller
    - sed -i "s/ARAX_BRANCH/${CI_BUILD_REF_NAME}/g" docker/Dockerfile
    - echo "${DOCK_PASS}" | buildah login -u "${DOCK_USER}" --password-stdin ${DOCK_HOST}
    - buildah build --target ${CI_PROJECT_NAME}controller-devel -t ${DEVEL_TAG} --volume '/usr/lib/libcuda.so:/usr/lib/libcuda.so' -f ./docker/Dockerfile .
    - buildah build --target ${CI_PROJECT_NAME}controller-runtime -t ${RUNTIME_TAG} --volume '/usr/lib/libcuda.so:/usr/lib/libcuda.so' -f ./docker/Dockerfile .
    - buildah build --target ${CI_PROJECT_NAME}controller-install -t ${INSTALL_TAG} --volume '/usr/lib/libcuda.so:/usr/lib/libcuda.so' -f ./docker/Dockerfile .
    - buildah push -q ${DEVEL_TAG}
    - buildah push -q ${RUNTIME_TAG}
    - buildah push -q ${INSTALL_TAG}
    - buildah rm -a
    - buildah rmi -a

package_build:
  <<: *only_master
  stage: build
  needs: []
  script:
    - tar czvf ${CI_PROJECT_NAME}_${CI_BUILD_REF_NAME}.tgz *
  artifacts:
    paths:
      - ${CI_PROJECT_NAME}_${CI_BUILD_REF_NAME}.tgz
    expire_in: 1 Day

package_publish:
  <<: *only_master
  stage: publish
  needs: [package_build]
  script:
    - 'curl --header "JOB-TOKEN: $CI_JOB_TOKEN" --upload-file ${CI_PROJECT_NAME}_${CI_BUILD_REF_NAME}.tgz "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${CI_PROJECT_NAME}/${CI_BUILD_REF_NAME}/${CI_PROJECT_NAME}_${CI_BUILD_REF_NAME}.tgz"'
  dependencies:
    - package_build

controller:
  stage: test
  needs: []
  script:
    - echo "shm_file /dev/shm/${CI_JOB_TOKEN}" > ~/.arax
    - echo -e "\e[0Ksection_start:`date +%s`:vtbuild[collapsed=true]\r\e[0KBuild Arax"
    - mkdir -p build
    - cd build
    - cmake .. -DBUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=DEBUG -DJAVA_WRAPS=ON -Dasync_architecture=mutex
    - make -j all noop noop_stress
    - make install
    - cp ../controller/ci/rr_cpu.json ./conf.json
    - make run &
    - sleep 1
    - gdb -batch -ex "set logging on" -ex "run" -ex "bt full" -ex "quit" --args ./noop/noop '!skrow C nialP'
    - ./araxgrind/araxgrind --all
    - java -jar JArax.jar '!skrow avaJ'
    - ./araxgrind/araxgrind --all
    - gdb -batch -ex "set logging on" -ex "run" -ex "bt full" -ex "quit" --args ./noop/noop_stress
    - killall -SIGINT arax_controller
    - sleep 1
  after_script:
    - rm /dev/shm/${CI_JOB_TOKEN}
  retry: 2

trigger_autotalk:
  stage: trigger_deps
  needs: [publish_controller_docker]
  trigger:
    project: accelerators/auto_talk
    branch: ${CI_BUILD_REF_NAME}

trigger_cufft:
  stage: trigger_deps
  needs: [publish_controller_docker]
  trigger:
    project: accelerators/arax_fft
    branch: ${CI_BUILD_REF_NAME}

report:
  stage: report
  script:
    - python ./ci_scripts/report.py
  when: always

release:
  stage: release
  parallel:
    matrix:
      - REPO: ['carvgit.ics.forth.gr:accelerators/internal_release.git']
  script:
    - git config --global user.email "${CI_EMAIL}"
    - git config --global user.name "${CI_USERNAME}"
    - git remote rm origin && git remote add origin git@${REPO}
    - git push origin HEAD:master
